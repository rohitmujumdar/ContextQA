# Using enhanced BiDAF++ on QuAC

## Descriptions
An original BiDAF++ model uses Char-CNN for character embedding and GLoVe for word embedding. It is also equipped with contextualized embeddings and self attention. In this model, marker embeddings corresponding to previous answer words are used, while question turn numbers are encoded into question embeddings.

We've used [AllenNLP](https://github.com/allenai/allennlp) library to modify the BiDAF++ and used enhenced models to train on [QuAC](https://quac.ai/) dataset. 

## Architecture of enhenced BiDAF++
<p align="center">
    <img src="Figures/Arch.png" width="90%"/>
</p>

### Embedding layers from BiDAF++

We aimed to apply ELMo or BERT embedding to the original embeddings.


Let ELMo<sub>k</sub> be a ELMo vector, x<sub>k</sub> denote a original embedding vector and h<sub>k</sub> present a contextual vector generated by bi-LSTM layer. Then we can use ELMo enhanced representation [x<sub>k</sub>; ELMo<sub>k</sub>] instead of x<sub>k</sub>. Also, replace h<sub>k</sub> with [h<sub>k</sub>; ELMo<sub>k</sub>].
We do the same thing when using BERT embedding.

ELMo ver:[options_file](https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_options.json)
      and [weight_file](https://s3-us-west-2.amazonaws.com/allennlp/models/elmo/2x4096_512_2048cnn_2xhighway/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5)  
BERT ver: [bert-large-cased](https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased-vocab.txt)
### Self-Attention layers from BiDAF++

First, the input is passed through a linear layer with ReLU activations. Then, it passes through a bi-directional GRU or LSTM. After that, we apply attention mechanism between the context and itself as following:

Let h<sub>i</sub>, h<sub>j</sub> be the vectors for context word i and word j and nc be the lengths of the context. Then compute attention between the two words as:
<p align="center">
    <img src="Figures/1.png" width="400"/>
</p>

where w<sub>1</sub>,w<sub>2</sub>, and w<sub>3</sub> are learned vectors and ⊙ is element-wise multiplica- tion. In this case, we set a<sub>ij</sub> = −inf if i = j. Then, compute an attended vector c<sub>i</sub> for each context token as:
<p align="center">
    <img src="Figures/2.png" width="350"/>
</p>

Compute a context-to-context vector t<sub>c</sub>:
<p align="center">
    <img src="Figures/3.png" width="450"/>
</p>

The output of this layer is built concatenating h<sub>i</sub>, c<sub>i</sub>, h<sub>i</sub> ⊙ c<sub>i</sub> and t<sub>c</sub> ⊙ c<sub>i</sub>,
additionally summed with the input before ReLU.
### Modification

We modified the wordpiece indexer and bert token embedder to train the model with BERT embedding, cause origianl two files cannot handle the situation with 500 more tokens in the sentence. The modified files would not influence training process of model with other embeddings.

## Platform and tools
### Launching a Deep Learning VM on GCP
- 2 vCPUs with 13GB memory
- 1 NVIDIA Tesla V100 GPU
- PyTorch 1.0 + fast.ai 1.0(CUDA 10.0)
- Install NVIDIA GPU driver automatically on first startup 

### Setting up an AllenNLP virtual environment

1.  Create a Conda environment with Python 3.6

    ```bash
    conda create -n allennlp python=3.6
    ```

2.  Activate the Conda environment. You will need to activate the Conda environment in each terminal in which you want to use AllenNLP.

    ```bash
    source activate allennlp
    ```


## Usage

### Dataset
[Train data](https://s3.amazonaws.com/my89public/quac/train_5000.json)  
[Validation data](https://s3.amazonaws.com/my89public/quac/val.json)

### Training
Train a enhanced model with QuAC dataset which includes training and validation dataset
```
nohup allennlp train <your_path_to_jsonnet_file>  --serialization-dir <your_path_to_log_file> > output.log &
```
As [QuAC](https://arxiv.org/pdf/1808.07036.pdf) mentioned，Questions in the training set have one reference answer, while validation and test questions have five references each, which makes the F1 score on validation dataset has a higher score then that on training set.

Best model's configuration is [here](https://github.com/deepnlp-cs599-usc/quac/blob/master/BiDAF/BiDAFF%2B%2B_with_glove%2Bbert.jsonnet).
## Results
### F1 score
Enhanced model by BERT
<p align="center">
    <img src="Figures/enhenced.png" width="200%"/>
</p>
Enhanced model by ELMo 
<p align="center">
    <img src="Figures/enhenced_elmo.png" width="200%"/>
</p>
Baseline model
<p align="center">
    <img src="Figures/baseline.png" width="200%"/>
</p>

### Performance on baseline and enhanced models

| | F1 score on training set | F1 score on validation set|
| --- | --- | --- |
| Baseline model | 49.40 | 55.59 |
| Enhanced by ELMo | 49.40 | **58.40** |
| Enhanced by BERT | **53.05** | **60.04**|

## Related works

* [Bidirectional attention ﬂow for machine comprehension](https://arxiv.org/abs/1611.01603) by Minjoon Seo et. al.
* [Simple and effective multi-paragraph reading comprehension](https://arxiv.org/abs/1710.10723) by Christopher Clark et. al.
* [Deep contextualized word representations](https://arxiv.org/abs/1802.05365) by Matthew E. Peters et. al.
* [Bert: Pre-training of deep bidirectional transformers for language understanding](https://arxiv.org/abs/1810.04805) by  Jacob Devlin et. al.
